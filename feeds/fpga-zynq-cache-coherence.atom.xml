<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>j-marjanovic.io - FPGA, Zynq, Cache-Coherence</title><link href="www.j-marjanovic.io/" rel="alternate"></link><link href="www.j-marjanovic.io/feeds/fpga-zynq-cache-coherence.atom.xml" rel="self"></link><id>www.j-marjanovic.io/</id><updated>2021-12-29T18:00:00+01:00</updated><entry><title>Exploring the PS-PL AXI interfaces on Zynq UltraScale+ MPSoC</title><link href="www.j-marjanovic.io/exploring-the-ps-pl-axi-interfaces-on-zynq-ultrascale-mpsoc.html" rel="alternate"></link><published>2021-12-29T18:00:00+01:00</published><updated>2021-12-29T18:00:00+01:00</updated><author><name>Jan Marjanovic</name></author><id>tag:None,2021-12-29:www.j-marjanovic.io/exploring-the-ps-pl-axi-interfaces-on-zynq-ultrascale-mpsoc.html</id><summary type="html">&lt;p&gt;I recently held &lt;a href="https://indico.desy.de/event/31387/contributions/112660/attachments/70411/89645/mtcaws2021_marjanovic_soc_amcs.pdf"&gt;a presentation at the 10th MicroTCA Workshop for Industry and
Research&lt;/a&gt;, where I
presented some hardware we have developed, discussed the advantages of using
SoCs, and showed a couple of examples where we successfully leveraged the features
of these devices.&lt;/p&gt;
&lt;p&gt;At my day job the dataflow through â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I recently held &lt;a href="https://indico.desy.de/event/31387/contributions/112660/attachments/70411/89645/mtcaws2021_marjanovic_soc_amcs.pdf"&gt;a presentation at the 10th MicroTCA Workshop for Industry and
Research&lt;/a&gt;, where I
presented some hardware we have developed, discussed the advantages of using
SoCs, and showed a couple of examples where we successfully leveraged the features
of these devices.&lt;/p&gt;
&lt;p&gt;At my day job the dataflow through the system is relatively simple, the data
is captured in large chunks and the direction is always well defined, i.e.
for data acquisition systems from FPGA to CPU and then into the network/storage.&lt;/p&gt;
&lt;p&gt;On the other hand, there are a lot of applications that can benefit from a
tighter coupling between the CPU and the FPGA, for example: HW-accelerated
compression and decompression, HW-accelerated regex, HW-accelerated graph
traversal, machine learning accelerators,...&lt;/p&gt;
&lt;p&gt;This is why I have decided to explore all ports available in Zynq US+ MPSoC in
my free time and to describe this adventure in this blog post.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;Xilinx Zynq UltraScale+ MPSoC provides four different types of interfaces
between the so-called &lt;em&gt;Processing System (PS)&lt;/em&gt; and &lt;em&gt;Programmable Logic (PL)&lt;/em&gt;,
leveraging the wide variety of different protocols standardized in &lt;a href="https://developer.arm.com/architectures/system-architectures/amba"&gt;Advanced
Microcontroller Bus
Architecture&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this blog post I will explore the performance characteristics of three
different interfaces:  &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accelerator Coherency Port interface&lt;/li&gt;
&lt;li&gt;High-Performance Coherent interface&lt;/li&gt;
&lt;li&gt;High-Performance interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;AXI Coherency Extension&lt;/em&gt; port is the most interesting of all and it deserves
a dedicated blog post.&lt;/p&gt;
&lt;p&gt;The purpose of this blog post is to gather information on how to make the
interfaces work correctly and to measure the performance in different scenarios.
The information on various interfaces is scattered across different documents
(&lt;a href="https://www.xilinx.com/support/documentation/user_guides/ug1085-zynq-ultrascale-trm.pdf"&gt;Zynq UltraScale+ Device Technical Reference
Manual&lt;/a&gt;,
&lt;a href="https://developer.arm.com/documentation/ddi0470/"&gt;CoreLink CCI-400 Cache Coherent Interconnect Technical Reference
Manual&lt;/a&gt;, and &lt;a href="https://developer.arm.com/documentation/ddi0500/"&gt;Arm Cortex-A53
MPCore Processor Technical Reference
Manual&lt;/a&gt;), &lt;a href="https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18842098/Zynq+UltraScale+MPSoC+Cache+Coherency"&gt;Xilinx wiki
pages&lt;/a&gt;,
&lt;a href="https://www.xilinx.com/support/answers/69446.html"&gt;Xilinx Answer Records&lt;/a&gt; and
various forum threads and GitHub issues.&lt;/p&gt;
&lt;h2&gt;PL-PS Interfaces&lt;/h2&gt;
&lt;p&gt;Shown in the figure below are the interfaces between PS and PL.&lt;/p&gt;
&lt;p&gt;&lt;img alt="PS-PL AXI Interface Datapaths - from UG1085 (courtesy Xilinx)" src="www.j-marjanovic.io/images/2021_zynqmp_ports/intro/ug1085_ps_pl_axi_interface.png" style="width:40%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;p&gt;In this blog post I will focus on the following three interfaces:&lt;/p&gt;
&lt;h3&gt;High-Performance interface&lt;/h3&gt;
&lt;p&gt;This interface connects directly to the &lt;em&gt;DDR Memory Subsystem&lt;/em&gt; and completely
bypasses the Cache Coherent Interconnect and the APU. According to UG1085, this
interface is ideal for large datasets. The software needs to bypass the
cache when accessing the data.&lt;/p&gt;
&lt;h3&gt;High-Performance Coherent interface&lt;/h3&gt;
&lt;p&gt;This interface is connected to a port on CCI-400 interconnect. When configured
accordingly the memory transactions are communicated to the APU, providing
tighter integration with software.&lt;/p&gt;
&lt;p&gt;AR 69446 mentions that:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The HPC ports are preferable to the ACP port in most applications as they
provide higher bandwidth and do not disturb the contents of the processor L2
cache.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Accelerator Coherency Port interface&lt;/h3&gt;
&lt;p&gt;Cortex-A53 TRM describes the ACP interface in the following way:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ACP is provided to reduce software cache maintenance operations when
sharing memory regions with other masters, and to allow other masters to
allocate data into the L2 cache.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;According to UG1085:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[...] ACP is optimal for medium-grain acceleration, such as a
block-level crypto accelerator and video macro-block level processing.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Cache-aware interfaces allow tighter integration between SW and HW which is of
particular interest for accelerating workloads with FPGA. Instead of explicitly
copying the data to the FPGA, doing the computation in the FPGA, and copying
data back from the FPGA, with cache-aware interfaces we can modify the data
directly in the program memory, thus skipping the unnecessary copying.&lt;/p&gt;
&lt;p&gt;I have prepared &lt;a href="https://github.com/j-marjanovic/meta-zynqmp-pl-ps-interfaces/blob/rel-v2021.1/recipes-app/numpy-acc/files/numpy_example.ipynb"&gt;a Jupyter
notebook&lt;/a&gt;
that demonstrates this approach. An AXI Proxy (explained in detail in the
next chapter)  is used to read from and to a Numpy array.&lt;/p&gt;
&lt;p&gt;The screenshot below shows the most important part of the notebook, where the
AXI Proxy is used to modify the content of the Numpy array from the FPGA side.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Writing to a Numpy array" src="www.j-marjanovic.io/images/2021_zynqmp_ports/motivation/numpy_acc.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h1&gt;Test setup&lt;/h1&gt;
&lt;h2&gt;Hardware&lt;/h2&gt;
&lt;p&gt;All measurements were performed on an Ultra96-V2 board. The board contains an
XCZU3EG, 2GB of DDR4 memory connected to the Processing System, and not much
more.&lt;/p&gt;
&lt;p&gt;The blue LED near the USB connector is used to indicate activity on AXI PL-PS ports:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Ultra96-V2" src="www.j-marjanovic.io/images/2021_zynqmp_ports/intro/u96.jpg" style="width:50%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h2&gt;AXI Proxy&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/j-marjanovic/chisel-stuff/tree/master/example-12-axi-proxy"&gt;AXI Proxy
IP&lt;/a&gt;
acts (as the name suggests) as a proxy between an AXI4-Lite subordinate port and
an AXI4 manager port. With this IP we can measure read and write latency on
all aforementioned ports.&lt;/p&gt;
&lt;p&gt;The subordinate port provides registers where the software can prepare the data
to be written, retrieve the data which was read, start read and write
transactions and measure the time an individual transaction took. The
transaction time is measured in clock cycles between an address being provided
(&lt;code&gt;AxVALID&lt;/code&gt; going high) and a full response is received. To match the
requirements of the ACP interface, all transactions are 64-byte long (4 beats,
128-bit wide). The software has also the possibility to set some of the &lt;a href="https://github.com/j-marjanovic/chisel-stuff/blob/master/example-12-axi-proxy/src/main/scala/axi_proxy/AxiProxy.scala#L136"&gt;AXI
side-band
signals&lt;/a&gt;
which affect the caching properties: &lt;code&gt;AxCACHE&lt;/code&gt;, &lt;code&gt;AxPROT&lt;/code&gt;, and &lt;code&gt;AxUSER&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Shown in the figure below is the Vivado block diagram used to perform the tests
with &lt;strong&gt;AXI Proxy&lt;/strong&gt;. There are three instances of the IP, each connected to one
of the ports on the Zynq MPSoC block. System ILA is used to provide additional
visibility of the connections between AXI Proxy and PL-PS ports on the Zynq
UltraScale+ MPSoC block.&lt;/p&gt;
&lt;p&gt;&lt;img alt="AXI Proxy" src="www.j-marjanovic.io/images/2021_zynqmp_ports/intro/vivado_axi_proxy.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h2&gt;AXI Traffic Generator&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://github.com/j-marjanovic/chisel-stuff/tree/master/example-13-axi-traffic-gen"&gt;AXI Traffic
Generator&lt;/a&gt;
can generate a sequence of AXI bursts with an incrementing address and a known
pattern, which simulates behavior of a DMA. With this IP we can measure the read
and write throughput of transactions of different sizes.&lt;/p&gt;
&lt;p&gt;Also here each AXI burst contains 64 bytes (4 beats, 128-bit). The IP measures
the number of clock cycles the entire transfer took.&lt;/p&gt;
&lt;p&gt;Shown in the figure below is the Vivado block diagram used to perform the tests
with &lt;strong&gt;Jan's AXI Traffic Generator&lt;/strong&gt;. As with the AXI Proxy, there are three
instances and a System ILA to observe the transactions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="AXI Traffic Generator" src="www.j-marjanovic.io/images/2021_zynqmp_ports/intro/vivado_axi_tg.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h2&gt;Yocto layer&lt;/h2&gt;
&lt;p&gt;To generate the SD card image containing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the FPGA bitstreams, &lt;/li&gt;
&lt;li&gt;FPGA manager (to download the bitstreams), &lt;/li&gt;
&lt;li&gt;device tree overlays (the description of FPGA),&lt;/li&gt;
&lt;li&gt;programs to control the IPs, &lt;/li&gt;
&lt;li&gt;the required Python libraries and &lt;/li&gt;
&lt;li&gt;Jupyter notebooks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I have created a Yocto layer, available on my GitHub:
&lt;a href="https://github.com/j-marjanovic/meta-zynqmp-pl-ps-interfaces"&gt;meta-zynqmp-pl-ps-interfaces&lt;/a&gt;.
The usage description is available in the &lt;code&gt;README.md&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;Configuration&lt;/h2&gt;
&lt;h3&gt;u-dma-buf&lt;/h3&gt;
&lt;p&gt;For use with ACP and HPC port, the u-dma-buffer needs to have the
&lt;a href="https://github.com/ikwzm/udmabuf#dma-coherent"&gt;&lt;code&gt;dma-coherent&lt;/code&gt;&lt;/a&gt; flag set.
This is achieved with &lt;a href="https://github.com/j-marjanovic/meta-zynqmp-pl-ps-interfaces/blob/rel-v2021.1/recipes-bsp/device-tree/files/app-pl-custom.dtsi#L6-L12"&gt;an entry&lt;/a&gt; in the device tree.&lt;/p&gt;
&lt;h3&gt;HP port&lt;/h3&gt;
&lt;p&gt;HP port requires no special configuration on the FPGA side, and the &lt;code&gt;u-dma-buf&lt;/code&gt;
needs to be opened with the &lt;a href="https://github.com/ikwzm/udmabuf#when-hardware-does-not-maintain-the-coherency"&gt;&lt;code&gt;O_SYNC&lt;/code&gt; flag&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;HPC port&lt;/h3&gt;
&lt;h4&gt;Addressing&lt;/h4&gt;
&lt;p&gt;The HPC by default uses physical addressing, although the SMMU can be configured
to use the virtual addressing, as mentioned in UG1085:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"Comparably, S_AXI_HPCx_FPD uses a virtual address [...]"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4&gt;FSBL and reg.init&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/j-marjanovic/meta-zynqmp-pl-ps-interfaces/tree/rel-v2021.1/recipes-bsp/fsbl/files"&gt;A couple of
patches&lt;/a&gt;
for the FSBL configure CCI in the correct configuration so that the
transactions on the HPC port are considered shareable.&lt;/p&gt;
&lt;h3&gt;ACP port&lt;/h3&gt;
&lt;p&gt;The ACP interface is in detail described in &lt;a href="https://developer.arm.com/documentation/ddi0500/e/level-2-memory-system/acp"&gt;ARM Cortex-A53 MPCore Processor
Technical Reference
Manual&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since this interface provides direct access into L2 cache, and therefore the
transfer needs to follow certain restrictions: to achieve the best performance
the transfers should be 64 bytes long (one cache line), and &lt;code&gt;AxCACHE&lt;/code&gt; and
&lt;code&gt;AxPROT&lt;/code&gt; signals need to be set to certain values.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://developer.arm.com/documentation/ihi0022/e/AMBA-AXI3-and-AXI4-Protocol-Specification/Transaction-Attributes/Memory-types"&gt;Memory types&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.arm.com/documentation/ihi0022/e/AMBA-AXI3-and-AXI4-Protocol-Specification/Transaction-Attributes/Access-permissions"&gt;Access permissions&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://developer.arm.com/documentation/ddi0500/e/level-2-memory-system/acp/acp-user-signals"&gt;ACP user signals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;regs.init&lt;/h4&gt;
&lt;p&gt;&lt;a href="https://github.com/j-marjanovic/meta-zynqmp-pl-ps-interfaces/blob/rel-v2021.1/recipes-bsp/bootbin/files/regs.init"&gt;regs.init&lt;/a&gt; is used to write to the APU Configuration Register (&lt;code&gt;LPD_SLCR&lt;/code&gt;), which
enables the broadcasting of the transactions towards the CCI, as described on
&lt;a href="https://xilinx-wiki.atlassian.net/wiki/spaces/A/pages/18842098/Zynq+UltraScale+MPSoC+Cache+Coherency#ZynqUltraScaleMPSoCCacheCoherency-5.2.2RegisterWriteAtEarlyBoot"&gt;Xilinx Wiki on Cache
Coherence&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Measurements&lt;/h1&gt;
&lt;h2&gt;Throughput/interface utilization&lt;/h2&gt;
&lt;p&gt;The throughput was measured with the AXI Traffic Generator, described in one of the
previous chapters. What we measure here is not strictly throughput but
interface utilization, i.e. in what percentage of the clock cycles was there a
beat transmitted on the port. One can easily derive the throughput (in B/s) from
the interface parameters (16-byte wide, running at 250 MHz). During the tests,
only one interface was active at the time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The entire measurement procedure is documented in a Jupyter notebook: &lt;a href="https://github.com/j-marjanovic/meta-zynqmp-pl-ps-interfaces/blob/rel-v2021.1/recipes-app/apps-pl-ps-interfaces/files/notebooks/00-traffic-gen.ipynb"&gt;00-traffic-gen.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The FPGA project for Ultra96-V2 board is available here: &lt;a href="https://github.com/j-marjanovic/chisel-stuff/tree/master/example-13-axi-traffic-gen/ultra96v2_prj"&gt;example-13-axi-traffic-gen/ultra96v2_prj&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final results are presented in the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Interface utilization measurement" src="www.j-marjanovic.io/images/2021_zynqmp_ports/meas/throughput_250MHz.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;p&gt;The HP port exhibits a typical behavior of a process where the start-up time
plays a significant role - the interface utilization is lower for smaller
transactions, but approaches an asymptotical limit at a certain size. All ports
use the same settings (4 transactions in flight, 64 bytes per transaction) to
make the comparison between different ports fair; this value is clearly too
low for the long latencies of DDR4. To achieve higher throughput, one should
increase the burst length.&lt;/p&gt;
&lt;p&gt;CCI adds additional latency to the HPC port and the throughput on this port is
even lower. Interestingly, once the transfer size is larger than the L2 cache,
the interface utilization starts getting higher. This would indicate that the
cache switches to write-through "mode" once it has seen a certain access
pattern; the CCI400 TRM is quite vague in this regard.&lt;/p&gt;
&lt;p&gt;The ACP benefits from the L1 and L2 caches and the interface utilization is the
highest of all ports for small transfers (below 1 MB). Once the transfer size
gets larger than the L2 cache size the interface utilization is severely
affected.&lt;/p&gt;
&lt;h2&gt;Latency&lt;/h2&gt;
&lt;p&gt;The latency was measured with AXI Proxies. A single measurement sample was
obtained with the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the SW generates a random value and instructs AXI Proxy to write this random
   value into a shared buffer through one of the ports&lt;/li&gt;
&lt;li&gt;the SW then reads the data from the buffer and compares it to what AXI
   Proxy has written&lt;/li&gt;
&lt;li&gt;the SW then generates another random value and writes it directly to the
   buffer&lt;/li&gt;
&lt;li&gt;the SW then instructs the AXI Proxy to read the data and then compares the
   value to the previously generated value&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With this procedure we can check that the writes and reads are visible in both
directions, and this procedure also mimics a typical usage of an FPGA
accelerator. During the tests, only one interface was active at the time.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The entire measurement procedure is documented in a Jupyter notebook: &lt;a href="https://github.com/j-marjanovic/meta-zynqmp-pl-ps-interfaces/blob/rel-v2021.1/recipes-app/apps-pl-ps-interfaces/files/notebooks/02-axi-proxy-on-repeat.ipynb"&gt;02-axi-proxy-on-repeat.ipynb&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;The FPGA project for Ultra96 board is available here: &lt;a href="https://github.com/j-marjanovic/chisel-stuff/tree/master/example-12-axi-proxy/ultra96v2_prj"&gt;example-12-axi-proxy/ultra96v2_prj&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The results of the latency measurements are presented in the graph below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Latency measurement" src="www.j-marjanovic.io/images/2021_zynqmp_ports/meas/latency_250MHz.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;p&gt;On the cached interfaces (ACP and HPC) we see, expectedly, very uniform latency
values. On the other side, on the HP interface we see that the latency to the
DDR4 is quite high and also quite variable, presumably because the transaction
needs to be scheduled after some other transactions or memory refresh is
currently being performed.&lt;/p&gt;
&lt;p&gt;There is an interesting start-up behavior on the ACP and HPC interfaces. I am
assuming that the L2 cache is an &lt;em&gt;inclusive cache&lt;/em&gt; and contains all lines from
the L1 cache. When a write request arrives at the L2 cache and the line is in
&lt;em&gt;Invalid&lt;/em&gt; state, the L2 can immediately decide that it can acknowledge the write
transaction. When the write request arrives at the L1 cache, however, the L1
cache needs to first inform the L2 cache to invalidate the line, and only then
proceed by acknowledging the write transaction.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In this blog post I have explored three different types of interfaces between PL
and PS in Zynq UltraScale+ MPSoC. Different types of interfaces provide
different trade-offs in terms of coupling between SW/HW, ease of use,
throughput, and latency. Cache-aware interfaces can be used to access the data
structures in running software and can therefore be used to seamlessly
accelerate certain parts of the program.&lt;/p&gt;
&lt;p&gt;Some edge cases were not explored (e.g. increasing the burst length for the
interface utilization measurement), but I decided to skip those to make the blog
post short(er). With Zynq US+ MPSoC boards being ubiquitous, this task is "left
as an exercise to the reader".&lt;/p&gt;
&lt;p&gt;I hope that this blog post, alongside the Vivado projects and the Yocto layer
can be used as a reference on how to use those ports.&lt;/p&gt;
&lt;hr&gt;
&lt;h1&gt;Appendix&lt;/h1&gt;
&lt;h2&gt;System ILA waveforms&lt;/h2&gt;
&lt;h3&gt;AXI Proxy&lt;/h3&gt;
&lt;h4&gt;HP port&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;app-axi-proxy --interface hp --use-osync
&lt;span class="go"&gt;Udmabuf&lt;/span&gt;
&lt;span class="go"&gt;  name = axi:udmabuf@0x0&lt;/span&gt;
&lt;span class="go"&gt;  virt addr = 0xffff9c0c7000&lt;/span&gt;
&lt;span class="go"&gt;  phys addr = 0x5e100000&lt;/span&gt;
&lt;span class="go"&gt;  size = 33554432&lt;/span&gt;
&lt;span class="go"&gt;  flags = 0x101002&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  5, name =             AxiProxy, addr = 0xa0020000, size = 65536, note = hp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  6, name =             AxiProxy, addr = 0xa0030000, size = 65536, note = hpc}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  4, name =             AxiProxy, addr = 0xa0040000, size = 65536, note = acp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  1, name =             axi-pmon, addr = 0xfd0b0000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  2, name =             axi-pmon, addr = 0xfd490000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  0, name =             axi-pmon, addr = 0xffa00000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  3, name =             axi-pmon, addr = 0xffa10000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;AxiProxy info:&lt;/span&gt;
&lt;span class="go"&gt;  id reg = 0xa8122081&lt;/span&gt;
&lt;span class="go"&gt;  version = 0.3.1&lt;/span&gt;
&lt;span class="go"&gt;SW read, HW written:&lt;/span&gt;
&lt;span class="go"&gt;  10, 10&lt;/span&gt;
&lt;span class="go"&gt;  20, 20&lt;/span&gt;
&lt;span class="go"&gt;  30, 30&lt;/span&gt;
&lt;span class="go"&gt;  40, 40&lt;/span&gt;
&lt;span class="go"&gt;readback, expected:&lt;/span&gt;
&lt;span class="go"&gt;  10, 10&lt;/span&gt;
&lt;span class="go"&gt;  20, 20&lt;/span&gt;
&lt;span class="go"&gt;  30, 30&lt;/span&gt;
&lt;span class="go"&gt;  40, 40&lt;/span&gt;
&lt;span class="go"&gt;stats: dur_wr = 36, dur_rd = 70&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="AXI Proxy read and write - HP port" src="www.j-marjanovic.io/images/2021_zynqmp_ports/system_ila/proxy_hp.png" style="width:100%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h4&gt;HPC port&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;app-axi-proxy --interface hpc --axi-cache &lt;span class="m"&gt;15&lt;/span&gt; --axi-prot &lt;span class="m"&gt;2&lt;/span&gt; --axi-user &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="go"&gt;Udmabuf&lt;/span&gt;
&lt;span class="go"&gt;  name = axi:udmabuf@0x0&lt;/span&gt;
&lt;span class="go"&gt;  virt addr = 0xffff931d8000&lt;/span&gt;
&lt;span class="go"&gt;  phys addr = 0x5e100000&lt;/span&gt;
&lt;span class="go"&gt;  size = 33554432&lt;/span&gt;
&lt;span class="go"&gt;  flags = 0x2&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  5, name =             AxiProxy, addr = 0xa0020000, size = 65536, note = hp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  6, name =             AxiProxy, addr = 0xa0030000, size = 65536, note = hpc}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  4, name =             AxiProxy, addr = 0xa0040000, size = 65536, note = acp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  1, name =             axi-pmon, addr = 0xfd0b0000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  2, name =             axi-pmon, addr = 0xfd490000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  0, name =             axi-pmon, addr = 0xffa00000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  3, name =             axi-pmon, addr = 0xffa10000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;AxiProxy info:&lt;/span&gt;
&lt;span class="go"&gt;  id reg = 0xa8122081&lt;/span&gt;
&lt;span class="go"&gt;  version = 0.3.1&lt;/span&gt;
&lt;span class="go"&gt;SW read, HW written:&lt;/span&gt;
&lt;span class="go"&gt;  10, 10&lt;/span&gt;
&lt;span class="go"&gt;  20, 20&lt;/span&gt;
&lt;span class="go"&gt;  30, 30&lt;/span&gt;
&lt;span class="go"&gt;  40, 40&lt;/span&gt;
&lt;span class="go"&gt;readback, expected:&lt;/span&gt;
&lt;span class="go"&gt;  10, 10&lt;/span&gt;
&lt;span class="go"&gt;  20, 20&lt;/span&gt;
&lt;span class="go"&gt;  30, 30&lt;/span&gt;
&lt;span class="go"&gt;  40, 40&lt;/span&gt;
&lt;span class="go"&gt;stats: dur_wr = 46, dur_rd = 40&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="AXI Proxy read and write - HPC port" src="www.j-marjanovic.io/images/2021_zynqmp_ports/system_ila/proxy_hpc.png" style="width:100%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h4&gt;ACP port&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;app-axi-proxy --interface acp --axi-cache &lt;span class="m"&gt;15&lt;/span&gt; --axi-prot &lt;span class="m"&gt;2&lt;/span&gt; --axi-user &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="go"&gt;Udmabuf&lt;/span&gt;
&lt;span class="go"&gt;  name = axi:udmabuf@0x0&lt;/span&gt;
&lt;span class="go"&gt;  virt addr = 0xffffa1356000&lt;/span&gt;
&lt;span class="go"&gt;  phys addr = 0x5e100000&lt;/span&gt;
&lt;span class="go"&gt;  size = 33554432&lt;/span&gt;
&lt;span class="go"&gt;  flags = 0x2&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  5, name =             AxiProxy, addr = 0xa0020000, size = 65536, note = hp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  6, name =             AxiProxy, addr = 0xa0030000, size = 65536, note = hpc}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  4, name =             AxiProxy, addr = 0xa0040000, size = 65536, note = acp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  1, name =             axi-pmon, addr = 0xfd0b0000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  2, name =             axi-pmon, addr = 0xfd490000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  0, name =             axi-pmon, addr = 0xffa00000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  3, name =             axi-pmon, addr = 0xffa10000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;AxiProxy info:&lt;/span&gt;
&lt;span class="go"&gt;  id reg = 0xa8122081&lt;/span&gt;
&lt;span class="go"&gt;  version = 0.3.1&lt;/span&gt;
&lt;span class="go"&gt;SW read, HW written:&lt;/span&gt;
&lt;span class="go"&gt;  10, 10&lt;/span&gt;
&lt;span class="go"&gt;  20, 20&lt;/span&gt;
&lt;span class="go"&gt;  30, 30&lt;/span&gt;
&lt;span class="go"&gt;  40, 40&lt;/span&gt;
&lt;span class="go"&gt;readback, expected:&lt;/span&gt;
&lt;span class="go"&gt;  10, 10&lt;/span&gt;
&lt;span class="go"&gt;  20, 20&lt;/span&gt;
&lt;span class="go"&gt;  30, 30&lt;/span&gt;
&lt;span class="go"&gt;  40, 40&lt;/span&gt;
&lt;span class="go"&gt;stats: dur_wr = 25, dur_rd = 14&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="AXI Proxy read and write - ACP port" src="www.j-marjanovic.io/images/2021_zynqmp_ports/system_ila/proxy_acp.png" style="width:100%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h3&gt;&lt;/h3&gt;
&lt;h4&gt;HP port&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;app-axi-traffic-gen --count &lt;span class="m"&gt;32&lt;/span&gt; --interface hp --use-osync
&lt;span class="go"&gt;UioDevice{number =  4, name =        AxiTrafficGen, addr = 0xa0040000, size = 65536, note = acp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  5, name =        AxiTrafficGen, addr = 0xa0050000, size = 65536, note = hp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  6, name =        AxiTrafficGen, addr = 0xa0060000, size = 65536, note = hpc}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  1, name =             axi-pmon, addr = 0xfd0b0000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  2, name =             axi-pmon, addr = 0xfd490000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  0, name =             axi-pmon, addr = 0xffa00000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  3, name =             axi-pmon, addr = 0xffa10000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;AxiTrafficGen info:&lt;/span&gt;
&lt;span class="go"&gt;  id reg = 0xa8172a9e&lt;/span&gt;
&lt;span class="go"&gt;  version = 0.9.7&lt;/span&gt;
&lt;span class="go"&gt;Udmabuf&lt;/span&gt;
&lt;span class="go"&gt;  name = axi:udmabuf@0x0&lt;/span&gt;
&lt;span class="go"&gt;  virt addr = 0xffff954fe000&lt;/span&gt;
&lt;span class="go"&gt;  phys addr = 0x5e100000&lt;/span&gt;
&lt;span class="go"&gt;  size = 33554432&lt;/span&gt;
&lt;span class="go"&gt;  flags = 0x101002&lt;/span&gt;
&lt;span class="go"&gt;Transfering 32 bursts&lt;/span&gt;
&lt;span class="go"&gt;Memory check (size = 32 bursts) successfully completed&lt;/span&gt;
&lt;span class="go"&gt;Memory check (size = 32 bursts) successfully completed&lt;/span&gt;
&lt;span class="go"&gt;stats: rd cyc = 670, wr cyc = 265, rd_ok = 128&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="AXI Traffic Generator write and read - HP port" src="www.j-marjanovic.io/images/2021_zynqmp_ports/system_ila/tg_hp.png" style="width:100%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h4&gt;HPC port&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;app-axi-traffic-gen --count &lt;span class="m"&gt;32&lt;/span&gt; --interface hpc --axi-cache &lt;span class="m"&gt;15&lt;/span&gt; --axi-prot &lt;span class="m"&gt;2&lt;/span&gt; --axi-user &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  4, name =        AxiTrafficGen, addr = 0xa0040000, size = 65536, note = acp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  5, name =        AxiTrafficGen, addr = 0xa0050000, size = 65536, note = hp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  6, name =        AxiTrafficGen, addr = 0xa0060000, size = 65536, note = hpc}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  1, name =             axi-pmon, addr = 0xfd0b0000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  2, name =             axi-pmon, addr = 0xfd490000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  0, name =             axi-pmon, addr = 0xffa00000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  3, name =             axi-pmon, addr = 0xffa10000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;AxiTrafficGen info:&lt;/span&gt;
&lt;span class="go"&gt;  id reg = 0xa8172a9e&lt;/span&gt;
&lt;span class="go"&gt;  version = 0.9.7&lt;/span&gt;
&lt;span class="go"&gt;Udmabuf&lt;/span&gt;
&lt;span class="go"&gt;  name = axi:udmabuf@0x0&lt;/span&gt;
&lt;span class="go"&gt;  virt addr = 0xffffa781e000&lt;/span&gt;
&lt;span class="go"&gt;  phys addr = 0x5e100000&lt;/span&gt;
&lt;span class="go"&gt;  size = 33554432&lt;/span&gt;
&lt;span class="go"&gt;  flags = 0x2&lt;/span&gt;
&lt;span class="go"&gt;Transfering 32 bursts&lt;/span&gt;
&lt;span class="go"&gt;Memory check (size = 32 bursts) successfully completed&lt;/span&gt;
&lt;span class="go"&gt;Memory check (size = 32 bursts) successfully completed&lt;/span&gt;
&lt;span class="go"&gt;stats: rd cyc = 391, wr cyc = 756, rd_ok = 128&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="AXI Traffic Generator write - HPC port" src="www.j-marjanovic.io/images/2021_zynqmp_ports/system_ila/tg_hpc_write.png" style="width:100%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="AXI Traffic Generator reads - HPC port" src="www.j-marjanovic.io/images/2021_zynqmp_ports/system_ila/tg_hpc_read.png" style="width:100%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h4&gt;ACP port&lt;/h4&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;app-axi-traffic-gen --count &lt;span class="m"&gt;32&lt;/span&gt; --interface acp --axi-cache &lt;span class="m"&gt;15&lt;/span&gt; --axi-prot &lt;span class="m"&gt;2&lt;/span&gt; --axi-user &lt;span class="m"&gt;1&lt;/span&gt;     
&lt;span class="go"&gt;UioDevice{number =  4, name =        AxiTrafficGen, addr = 0xa0040000, size = 65536, note = acp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  5, name =        AxiTrafficGen, addr = 0xa0050000, size = 65536, note = hp}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  6, name =        AxiTrafficGen, addr = 0xa0060000, size = 65536, note = hpc}&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  1, name =             axi-pmon, addr = 0xfd0b0000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  2, name =             axi-pmon, addr = 0xfd490000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  0, name =             axi-pmon, addr = 0xffa00000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;UioDevice{number =  3, name =             axi-pmon, addr = 0xffa10000, size = 65536, note = }&lt;/span&gt;
&lt;span class="go"&gt;AxiTrafficGen info:&lt;/span&gt;
&lt;span class="go"&gt;  id reg = 0xa8172a9e&lt;/span&gt;
&lt;span class="go"&gt;  version = 0.9.7&lt;/span&gt;
&lt;span class="go"&gt;Udmabuf&lt;/span&gt;
&lt;span class="go"&gt;  name = axi:udmabuf@0x0&lt;/span&gt;
&lt;span class="go"&gt;  virt addr = 0xffff9469a000&lt;/span&gt;
&lt;span class="go"&gt;  phys addr = 0x5e100000&lt;/span&gt;
&lt;span class="go"&gt;  size = 33554432&lt;/span&gt;
&lt;span class="go"&gt;  flags = 0x2&lt;/span&gt;
&lt;span class="go"&gt;Transfering 32 bursts&lt;/span&gt;
&lt;span class="go"&gt;Memory check (size = 32 bursts) successfully completed&lt;/span&gt;
&lt;span class="go"&gt;Memory check (size = 32 bursts) successfully completed&lt;/span&gt;
&lt;span class="go"&gt;stats: rd cyc = 140, wr cyc = 248, rd_ok = 128&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;img alt="AXI Traffic Generator reads - ACP port" src="www.j-marjanovic.io/images/2021_zynqmp_ports/system_ila/tg_acp.png" style="width:100%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h2&gt;FSBL output&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="go"&gt;Xilinx Zynq MP First Stage Boot Loader &lt;/span&gt;
&lt;span class="go"&gt;Release 2021.1   Jun  6 2021  -  07:07:32&lt;/span&gt;
&lt;span class="go"&gt;MultiBootOffset: 0x0&lt;/span&gt;
&lt;span class="go"&gt;Reset Mode      :       System Reset&lt;/span&gt;
&lt;span class="go"&gt;Platform: Silicon (4.0), Running on A53-0 (64-bit) Processor, Device Name: XCZU3EG&lt;/span&gt;
&lt;span class="go"&gt;SD0 Boot Mode &lt;/span&gt;
&lt;span class="go"&gt;PMU Firmware 2021.1     Jun  6 2021   07:07:32&lt;/span&gt;
&lt;span class="go"&gt;PMU_ROM Version: xpbr-v8.1.0-0&lt;/span&gt;
&lt;span class="go"&gt;Protection configuration applied&lt;/span&gt;
&lt;span class="go"&gt;EL = 3&lt;/span&gt;
&lt;span class="go"&gt;CCI_REG: register dump&lt;/span&gt;
&lt;span class="go"&gt;  offset 0 = 0&lt;/span&gt;
&lt;span class="go"&gt;  offset 10 = 0&lt;/span&gt;
&lt;span class="go"&gt;  offset 14 = 8000003F&lt;/span&gt;
&lt;span class="go"&gt;  offset 18 = 0&lt;/span&gt;
&lt;span class="go"&gt;  offset 1C = 0&lt;/span&gt;
&lt;span class="go"&gt;  offset 40 = 0&lt;/span&gt;
&lt;span class="go"&gt;CCI_REG: debug enable&lt;/span&gt;
&lt;span class="go"&gt;CCI_REG: register dump&lt;/span&gt;
&lt;span class="go"&gt;  offset 0 = 0&lt;/span&gt;
&lt;span class="go"&gt;  offset 10 = 0&lt;/span&gt;
&lt;span class="go"&gt;  offset 14 = 8000003F&lt;/span&gt;
&lt;span class="go"&gt;  offset 18 = 0&lt;/span&gt;
&lt;span class="go"&gt;  offset 1C = 0&lt;/span&gt;
&lt;span class="go"&gt;  offset 40 = 3&lt;/span&gt;
&lt;span class="go"&gt;CCI: enable snoop, ctrl before = C0000000&lt;/span&gt;
&lt;span class="go"&gt;CCI: enable snoop, ctrl after = C0000001&lt;/span&gt;
&lt;span class="go"&gt;CCI: shareable override reg - before = 0&lt;/span&gt;
&lt;span class="go"&gt;CCI: shareable override reg - after = 3&lt;/span&gt;
&lt;span class="go"&gt;Exit from FSBL &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;hr&gt;
&lt;div style="font-size: 90%;" &gt;
Xilinx, Inc. Xilinx, the Xilinx logo, Vivado, Zynq are trademarks of Xilinx in the United States and
other countries.
&lt;/div&gt;

&lt;div style="font-size: 90%;" &gt;
AMBA, ARM, Cortex and TrustZone are registered trademarks of ARM Limited (or its
subsidiaries) in the EU and/or elsewhere. CoreLink is  a trademark of ARM
Limited (or its subsidiaries) in the EU and/or elsewhere.
&lt;/div&gt;

&lt;div style="font-size: 90%;" &gt;
All trademarks and registered trademarks are the property of their respective owners.
&lt;/div&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="FPGA, Zynq, Cache-Coherence"></category></entry><entry><title>Performance counters in Cache Coherent Interconnect in Zynq MPSoC</title><link href="www.j-marjanovic.io/performance-counters-in-cache-coherent-interconnect-in-zynq-mpsoc.html" rel="alternate"></link><published>2021-06-12T20:00:00+02:00</published><updated>2021-06-12T20:00:00+02:00</updated><author><name>Jan Marjanovic</name></author><id>tag:None,2021-06-12:www.j-marjanovic.io/performance-counters-in-cache-coherent-interconnect-in-zynq-mpsoc.html</id><summary type="html">&lt;p&gt;In this blog post I describe my tinkering with the interconnect architecture of
the Xilinx Zynq MPSoC. I specifically focus on the Performance Monitoring Unit
(PMU) integrated into the Cache Coherence Interconnect (CCI).&lt;/p&gt;
&lt;p&gt;I believe that most of the readers of my blog are already familiar with Xilinx
ZynqÂ® UltraScale â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;In this blog post I describe my tinkering with the interconnect architecture of
the Xilinx Zynq MPSoC. I specifically focus on the Performance Monitoring Unit
(PMU) integrated into the Cache Coherence Interconnect (CCI).&lt;/p&gt;
&lt;p&gt;I believe that most of the readers of my blog are already familiar with Xilinx
ZynqÂ® UltraScale+â„¢ MPSoC, but for the sake of completeness let's do a quick
introduction. Zynq MPSoC is a programmable device, combining a quad-core ARM
Cortex-A53 (called Application Processing Unit (APU) in Xilinx-speak) and a
relatively large FPGA (called Programmable Logic (PL) in Xilinx-speak) in one
package. Sitting in between the two parts is an interconnect, more precisely
ARMÂ® CoreLinkâ„¢ CCI-400 Cache Coherent Interconnect. The majority of the
connections between the APU and PL go through this interconnect, which makes it
one of the more important parts of the Processing System (PS).&lt;/p&gt;
&lt;p&gt;In simple use cases the interconnect is mostly transparent for the users; from
the APU side, the memory transactions (i.e. reads and writes) come to the
interconnect and get routed to the appropriate output port (e.g. DDR controller
or PL manager ports - &lt;code&gt;M_AXI_HPMx_FPD&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The overview of the CCI is shown in the excerpt from the UG1085 below - the
CCI is shown prominently in its central location.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Excerpt from the UG1085 showing the interconnect" src="www.j-marjanovic.io/images/2021_cci_part_1/ug1085_ps_interconnect.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h1&gt;Linux driver for CCI PMU&lt;/h1&gt;
&lt;p&gt;Already provided in Linux is a driver for the Performance Monitoring Unit
(PMU) in the CCI. This driver is enabled with the &lt;code&gt;CONFIG_ARM_CCI_PMU&lt;/code&gt; variable,
and for Zynq MPSoC this option is by default already turned on.&lt;/p&gt;
&lt;p&gt;The driver prints a short message in the &lt;code&gt;dmesg&lt;/code&gt; to indicate that it was
successfully loaded:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;dmesg &lt;span class="p"&gt;|&lt;/span&gt; grep CCI
&lt;span class="go"&gt;[    3.218405] ARM CCI_400_r1 PMU driver probed&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can then use &lt;a href="https://perf.wiki.kernel.org/index.php/Main_Page"&gt;perf&lt;/a&gt; command
to list all performance counters available in the system, and among those there
are also listed those from the CCI-400:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;perf list

&lt;span class="go"&gt;List of pre-defined events (to be used in -e):&lt;/span&gt;

&lt;span class="go"&gt;[...]&lt;/span&gt;
&lt;span class="go"&gt;  CCI_400_r1/cycles/                                 [Kernel PMU event]&lt;/span&gt;
&lt;span class="go"&gt;[...]&lt;/span&gt;
&lt;span class="go"&gt;  CCI_400_r1/si_rrq_hs_any,source=?/                 [Kernel PMU event]&lt;/span&gt;
&lt;span class="go"&gt;[...]&lt;/span&gt;
&lt;span class="go"&gt;  CCI_400_r1/si_wrq_hs_any,source=?/                 [Kernel PMU event]&lt;/span&gt;
&lt;span class="go"&gt;[...]&lt;/span&gt;
&lt;span class="go"&gt;  CCI_400_r1/si_wrq_hs_write_unique,source=?/        [Kernel PMU event]&lt;/span&gt;
&lt;span class="go"&gt;  CCI_400_r1/si_wrq_stall_tt_full,source=?/          [Kernel PMU event]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Although at this stage the performance counters are already accessible from the
system, except for the &lt;code&gt;cycles&lt;/code&gt; counter no other counter is actually counting.&lt;/p&gt;
&lt;p&gt;It turns out that the CCI-400 IP has some external signals which can disable
the performance counters, even if the registers are properly configured.&lt;/p&gt;
&lt;p&gt;Presented below is a table from the &lt;a href="https://developer.arm.com/documentation/ddi0470/i/programmers-model/register-descriptions/performance-monitor-control-register--pmcr-?lang=en"&gt;CCI-400 Technical Reference
Manual&lt;/a&gt;
(&lt;a href="http://archive.today/2021.06.12-193819/https://developer.arm.com/documentation/ddi0470/i/programmers-model/register-descriptions/performance-monitor-control-register--pmcr-?lang=en"&gt;archive.today
link&lt;/a&gt;)
which shows that &lt;code&gt;NIDEN&lt;/code&gt; must be high for &lt;em&gt;Event counters&lt;/em&gt; to be
enabled.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Table describing the function of the NIDEN input" src="www.j-marjanovic.io/images/2021_cci_part_1/cci_manual.png" style="width:50%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h1&gt;Configuration&lt;/h1&gt;
&lt;p&gt;Looking at the register map for Zynq MPSoC in the
&lt;a href="https://www.xilinx.com/html_docs/registers/ug1087/ug1087-zynq-ultrascale-registers.html"&gt;UG1087&lt;/a&gt;
one can note that there are two modules associated with Cache Coherent
Interconnect.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Modules in the Zynq MPSoC register map" src="www.j-marjanovic.io/images/2021_cci_part_1/ug1087.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;p&gt;In my opinion, the documentation from Xilinx is a little bit vague, but I presume
that the CCI_REG acts as a GPIO which then drives the debug inputs on the CCI
module. Shown in the figure below is the CCI module together with this auxiliary
module. This block diagram is based on my current understanding.&lt;/p&gt;
&lt;p&gt;&lt;img alt="CCI and debug signals" src="www.j-marjanovic.io/images/2021_cci_part_1/cci_reg_block_diag.png" style="width:60%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;h2&gt;Accessing the configuration registers&lt;/h2&gt;
&lt;p&gt;With this in mind, one would be tempted to quickly change the value of NIDEN
input directly from user space with &lt;code&gt;devmem&lt;/code&gt; utility.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;devmem 0xFD5E0000
&lt;span class="go"&gt;Bus error&lt;/span&gt;
&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;devmem 0xFD5E0040
&lt;span class="go"&gt;Bus error&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here we encounter another issue, namely the CCI_REG is protected by the Xilinx
Memory Protection Unit (XMPU) and is only accessible from the secure
environment.&lt;/p&gt;
&lt;h2&gt;Exception Levels&lt;/h2&gt;
&lt;p&gt;There are &lt;a href="https://developer.arm.com/documentation/102412/0100/Privilege-and-Exception-levels"&gt;4 exception
levels&lt;/a&gt;
defined in ARMv8 architecture.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The user space runs in EL0&lt;/li&gt;
&lt;li&gt;The kernel space runs in EL1&lt;/li&gt;
&lt;li&gt;Hypervisors run in EL2&lt;/li&gt;
&lt;li&gt;Firmware runs in EL3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can get the current Exception Level by reading the &lt;a href="https://developer.arm.com/documentation/ddi0595/2020-12/AArch64-Registers/CurrentEL--Current-Exception-Level"&gt;CurrentEL&lt;/a&gt; register.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In user space this instruction throws &lt;code&gt;Illegal instruction&lt;/code&gt; - this is expected&lt;/li&gt;
&lt;li&gt;In kernel space the reported level is 1: &lt;code&gt;[ 1091.821735] jan-level: EL = 1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;In FSBL the reported level is 3:  &lt;code&gt;EL = 3&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Patch for the First Stage BootLoader (FSBL)&lt;/h2&gt;
&lt;p&gt;Since we now know that FSBL runs in EL3, and we need EL3 to access the CCI_REG
module, we can patch the FSBL to configure the appropriate registers before
continuing with the boot. In this way, the &lt;code&gt;NIDEN&lt;/code&gt; and also &lt;code&gt;SPIDEN&lt;/code&gt; signals
will be already set high before the Linux boots.&lt;/p&gt;
&lt;p&gt;I wrote the following patch and included it in the Bitbake recipe for the FSBL:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;From 08450fd4c18d11fedf196c65c22f8abf83a6cc2a Mon Sep 17 00:00:00 2001
From: Jan Marjanovic &amp;lt;jan.marjanovic@outlook.com&amp;gt;
Date: Thu, 10 Jun 2021 19:20:25 +0200
Subject: [PATCH] Enable CCI debug (NIDEN and SPINDEN on CCI-400)

&lt;span class="gd"&gt;---&lt;/span&gt;
 lib/sw_apps/zynqmp_fsbl/src/xfsbl_hooks.c | 39 +++++++++++++++++++++--
 1 file changed, 36 insertions(+), 3 deletions(-)

&lt;span class="gh"&gt;diff --git a/lib/sw_apps/zynqmp_fsbl/src/xfsbl_hooks.c b/lib/sw_apps/zynqmp_fsbl/src/xfsbl_hooks.c&lt;/span&gt;
&lt;span class="gh"&gt;index 80a1314203..b0030a1d67 100644&lt;/span&gt;
&lt;span class="gd"&gt;--- a/lib/sw_apps/zynqmp_fsbl/src/xfsbl_hooks.c&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/lib/sw_apps/zynqmp_fsbl/src/xfsbl_hooks.c&lt;/span&gt;
&lt;span class="gu"&gt;@@ -64,13 +64,46 @@ u32 XFsbl_HookAfterBSDownload(void )&lt;/span&gt;
 }
 #endif

&lt;span class="gi"&gt;+static void print_el(void) {&lt;/span&gt;
&lt;span class="gi"&gt;+   register uint64_t x0 __asm__ (&amp;quot;x0&amp;quot;);&lt;/span&gt;
&lt;span class="gi"&gt;+   __asm__ (&amp;quot;mrs x0, CurrentEL;&amp;quot; : : : &amp;quot;%x0&amp;quot;);&lt;/span&gt;
&lt;span class="gi"&gt;+   XFsbl_Printf(DEBUG_PRINT_ALWAYS, &amp;quot;EL = %x\r\n&amp;quot;, x0 &amp;gt;&amp;gt; 2);&lt;/span&gt;
&lt;span class="gi"&gt;+}&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+static void cci_reg_dump(void) {&lt;/span&gt;
&lt;span class="gi"&gt;+   // offsets from UG1087&lt;/span&gt;
&lt;span class="gi"&gt;+   uint64_t offsets[] = {0, 0x10, 0x14, 0x18, 0x1c, 0x40};&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+   XFsbl_Printf(DEBUG_PRINT_ALWAYS, &amp;quot;CCI_REG: register dump\r\n&amp;quot;);&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+   for (int i = 0; i &amp;lt; sizeof(offsets)/sizeof(*offsets); i++) {&lt;/span&gt;
&lt;span class="gi"&gt;+       uint64_t offs = offsets[i];&lt;/span&gt;
&lt;span class="gi"&gt;+       u32 val = XFsbl_In32(XPAR_PSU_CCI_REG_S_AXI_BASEADDR + offs);&lt;/span&gt;
&lt;span class="gi"&gt;+       XFsbl_Printf(DEBUG_PRINT_ALWAYS, &amp;quot;  offset %x = %x\r\n&amp;quot;,&lt;/span&gt;
&lt;span class="gi"&gt;+               offs, val);&lt;/span&gt;
&lt;span class="gi"&gt;+   }&lt;/span&gt;
&lt;span class="gi"&gt;+}&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+static void cci_reg_debug_enable(void) {&lt;/span&gt;
&lt;span class="gi"&gt;+   const uint64_t OFFS_CCI_MISC_CTRL = 0x40;&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+   const uint32_t CCI_MISC_CTRL_NIDEN_MASK = 0x2;&lt;/span&gt;
&lt;span class="gi"&gt;+   const uint32_t CCI_MISC_CTRL_SPIDEN_MASK = 0x1;&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+   XFsbl_Printf(DEBUG_PRINT_ALWAYS, &amp;quot;CCI_REG: debug enable\r\n&amp;quot;);&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+   XFsbl_Out32(XPAR_PSU_CCI_REG_S_AXI_BASEADDR + OFFS_CCI_MISC_CTRL,&lt;/span&gt;
&lt;span class="gi"&gt;+           CCI_MISC_CTRL_NIDEN_MASK | CCI_MISC_CTRL_SPIDEN_MASK);&lt;/span&gt;
&lt;span class="gi"&gt;+}&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
 u32 XFsbl_HookBeforeHandoff(u32 EarlyHandoff)
 {
    u32 Status = XFSBL_SUCCESS;

&lt;span class="gd"&gt;-   /**&lt;/span&gt;
&lt;span class="gd"&gt;-    * Add the code here&lt;/span&gt;
&lt;span class="gd"&gt;-    */&lt;/span&gt;
&lt;span class="gi"&gt;+   print_el();&lt;/span&gt;
&lt;span class="gi"&gt;+   cci_reg_dump();&lt;/span&gt;
&lt;span class="gi"&gt;+   cci_reg_debug_enable();&lt;/span&gt;
&lt;span class="gi"&gt;+   cci_reg_dump();&lt;/span&gt;

    return Status;
 }
&lt;span class="gd"&gt;--&lt;/span&gt;
2.25.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The following is then the output of the FSBL with the patch:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;Xilinx Zynq MP First Stage Boot Loader
Release 2020.2   Jun 10 2021  -  19:49:38
Reset Mode      :       System Reset
Platform: Silicon (4.0), Running on A53-0 (64-bit) Processor, Device Name: XCZU3EG
SD0 Boot Mode
PMU Firmware 2020.2     Jun  3 2021   19:28:36
PMU_ROM Version: xpbr-v8.1.0-0
Protection configuration applied
EL = 3
CCI_REG: register dump
  offset 0 = 0
  offset 10 = 0
  offset 14 = 8000003F
  offset 18 = 0
  offset 1C = 0
  offset 40 = 0
CCI_REG: debug enable
CCI_REG: register dump
  offset 0 = 0
  offset 10 = 0
  offset 14 = 8000003F
  offset 18 = 0
  offset 1C = 0
  offset 40 = 3
Exit from FSBL
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can note that the register at the offset &lt;code&gt;0x40&lt;/code&gt; has changed from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;3&lt;/code&gt;,
as we have requested.&lt;/p&gt;
&lt;h2&gt;Lamport's bakery algorithm&lt;/h2&gt;
&lt;p&gt;To provide a good test case for the cache coherent interconnect I have
implemented distributed counting (i.e. multiple workers share one counter),
and the synchronization is provided with &lt;a href="https://github.com/j-marjanovic/chisel-stuff/tree/master/example-10-lamports-bakery-algorithm"&gt;Lamport's bakery algorithm&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;AXI protocol itself provides a possibility (&lt;code&gt;AxLOCK&lt;/code&gt; signals for exclusive
access) to perform atomic operations on memory and devices, but Lamport's
algorithm does not require any special locking primitives, &lt;a href="http://lamport.azurewebsites.net/pubs/pubs.html#bakery"&gt;only atomic reads
and writes&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="IPs connected to HPC ports on Zynq MPSoC" src="www.j-marjanovic.io/images/2021_cci_part_1/vivado.png" style="width:80%; display: block; margin-left: auto; margin-right: auto;"&gt;&lt;/p&gt;
&lt;p&gt;In the Vivado design I have connected two &lt;code&gt;LamportsBakeryAlgo&lt;/code&gt; IPs to both
HPC ports. Each IP can be configured to run for a defined number of loops,
and in each loop the counter will be incremented by 1. For each loop we
expect to see 5 writes in total:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;1 write for setting the entering flag&lt;/li&gt;
&lt;li&gt;1 write for setting the number&lt;/li&gt;
&lt;li&gt;1 write for clearing the entering flag&lt;/li&gt;
&lt;li&gt;1 write to do the actual work (increment the counter in this example)&lt;/li&gt;
&lt;li&gt;1 write to clear the number (i.e. release the lock)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Similarly, we can expect the following to be the lower limit of the number of reads
- if the flag is set the algorithm continues to poll it until it is cleared. In
this highly-contended example we expect the numbers of reads to be much higher.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;N reads for the &lt;code&gt;maximum()&lt;/code&gt; function&lt;/li&gt;
&lt;li&gt;N reads in the inner loop to check the entering flag&lt;/li&gt;
&lt;li&gt;N reads in the inner loop to check the number variable&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Performance counters usage example&lt;/h1&gt;
&lt;p&gt;With the Performance Memory Unit enabled we can now start monitoring the values
of the counters with the &lt;code&gt;perf&lt;/code&gt; tool.&lt;/p&gt;
&lt;p&gt;I ran the two &lt;code&gt;LamportsBakeryAlgo&lt;/code&gt; IP, each programmed to perform 100000 loops.
In addition, there is a third instance of Lamport's bakery algorithm, this
one running in the software and accessing the same memory locations as the two
FPGA implementations.&lt;/p&gt;
&lt;p&gt;In parallel, I ran &lt;code&gt;perf stat&lt;/code&gt; and selected the following events: read requests
(&lt;code&gt;rrq&lt;/code&gt;) handshakes (&lt;code&gt;hs&lt;/code&gt;) and write request (&lt;code&gt;wr&lt;/code&gt;) handshakes on subordinate
ports 0 and 3. From the
&lt;a href="https://www.xilinx.com/support/documentation/user_guides/ug1085-zynq-ultrascale-trm.pdf"&gt;UG1085&lt;/a&gt;
we can see that the APU is connected to port 3 on the CCI-400 and the HPC ports
are connected to port 0 on the interconnect.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;root@u96v2-sbc:~# &lt;/span&gt;perf stat -e &lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="gp"&gt;&amp;gt; &lt;/span&gt;CCI_400_r1/cycles/,&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="gp"&gt;&amp;gt; &lt;/span&gt;CCI_400_r1/si_rrq_hs_any,source&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;/,&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="gp"&gt;&amp;gt; &lt;/span&gt;CCI_400_r1/si_wrq_hs_any,source&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;0&lt;/span&gt;/,&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="gp"&gt;&amp;gt; &lt;/span&gt;CCI_400_r1/si_rrq_hs_any,source&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;/,&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="gp"&gt;&amp;gt; &lt;/span&gt;CCI_400_r1/si_wrq_hs_any,source&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;/

&lt;span class="go"&gt;^C&lt;/span&gt;
&lt;span class="go"&gt; Performance counter stats for &amp;#39;system wide&amp;#39;:&lt;/span&gt;

&lt;span class="go"&gt;        1103648324      CCI_400_r1/cycles/&lt;/span&gt;
&lt;span class="go"&gt;           3207971      CCI_400_r1/si_rrq_hs_any,source=0/&lt;/span&gt;
&lt;span class="go"&gt;           1000000      CCI_400_r1/si_wrq_hs_any,source=0/&lt;/span&gt;
&lt;span class="go"&gt;           3158786      CCI_400_r1/si_rrq_hs_any,source=3/&lt;/span&gt;
&lt;span class="go"&gt;            636417      CCI_400_r1/si_wrq_hs_any,source=3/&lt;/span&gt;

&lt;span class="go"&gt;       4.138963536 seconds time elapsed&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We see that expected 1000000 writes from the HPC ports (= 2 (instances) * 5 (per
loop) * 100000 (number of loops)) and we also see the number of reads on the HPS
port matches our expectation to be equal or greater than 1800000 (= 2 (total
instances) * 3 * 3 (total instances = N) * 100000 (number of loops)).&lt;/p&gt;
&lt;p&gt;Port 3 (APU port) is also used to load the program from
the main memory or the SD card which slightly obscures the number of
transactions performed by the algorithm itself, but the numbers match our
expectations.&lt;/p&gt;
&lt;h2&gt;Limitations&lt;/h2&gt;
&lt;p&gt;It seems that the PMU in the CCI-400 does not provide all facilities needed for
&lt;code&gt;perf record&lt;/code&gt; to work properly, it reports &lt;code&gt;PMU Hardware doesn't support
sampling/overflow-interrupts. Try 'perf stat'&lt;/code&gt;.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;In this post we have seen how tools commonly used in performance engineering
can also be used to observe the behavior and performance of the FPGA. With the
proliferation of heterogeneous architectures, having good tools to observe
(and debug) the interfaces between individual components provides additional
insight into the system.&lt;/p&gt;
&lt;p&gt;It is yet to be explored if shouting at the Zynq MPSoC has any impact
on the &lt;a href="https://www.youtube.com/watch?v=tDacjrSCeq4"&gt;performance in terms of
latency&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;div style="font-size: 80%;" &gt;
Xilinx, Inc. Xilinx, the Xilinx logo, Vivado, Zynq are trademarks of Xilinx in the United States and
other countries.
&lt;/div&gt;

&lt;div style="font-size: 80%;" &gt;
AMBA, ARM, Cortex and TrustZone are registered trademarks of ARM Limited (or its
subsidiaries) in the EU and/or elsewhere. CoreLink is  a trademark of ARM
Limited (or its subsidiaries) in the EU and/or elsewhere.
&lt;/div&gt;

&lt;div style="font-size: 80%;" &gt;
All trademarks and registered trademarks are the property of their respective owners.
&lt;/div&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="FPGA, Zynq, Cache-Coherence"></category></entry></feed>